<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <meta name="author" content="do.sibsutis.ru">

  <title>Лекция 3. Конспект лекций</title>
	
	<link rel="icon" type="image/png" href="../lib/css/favicon.png">
  <!-- Bootstrap Core CSS -->
  <link href="../lib/css/bootstrap.css" rel="stylesheet">
	
  <!-- Custom CSS -->
  <link href="../lib/css/scrolling-nav.css" rel="stylesheet">

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->
</head>

<!-- The #page-top ID is part of the scrolling feature - the data-spy and data-target are part of the built-in Bootstrap scrollspy function -->

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">
  <!-- Navigation -->
<nav class="navbar navbar-default navbar-fixed-top">
 <div class="container">
  <!-- Brand and toggle get grouped for better mobile display -->
  <div class="navbar-header">
   <li class="hidden"> <a class="page-scroll" href="#page-top"></a> </li>
	 
	 <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
    <span class="icon-bar"></span>
    <span class="icon-bar"></span>
    <span class="icon-bar"></span>
   </button>
	 
   <a class="navbar-brand " href="../index.htm"> 
  <text class = "hidden-xs">Защита информации</text> 
  <text class = "visible-xs">ЗИ</text>
	 </a>
  </div>

  <!-- Collect the nav links, forms, and glyphicon glyphicon-list-alt content for toggling -->
  <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
   
	 <!-- <ul class="nav navbar-nav">
		<li><a href="#"></a></li> 
   </ul> -->
	
   <ul class="nav navbar-nav navbar-right">
    <li class="dropdown">
		<button type="button" class="navbar-toggle dropdown-toggle hidden-xs" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
			<span class="icon-bar"></span>
			<span class="icon-bar"></span>
			<span class="icon-bar"></span>
		</button>	   
		<a class = "dropdown-toggle visible-xs" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Материалы</a>
     <ul class="dropdown-menu">
    <li><a href="../index.htm">Аннотация курса</a></li>
      <li role="separator" class="divider"></li>
  
 <li><a href="lec_index.htm">Теория</a></li>
   
  <li><a href="c_work.htm">Контрольная работа</a></li>

 
<li><a href="labs.htm">Лабораторные работы</a></li> 
      <li role="separator" class="divider"></li>
    
      <li><a href="lit.htm">Литература</a></li>
      <!--li><a href="q.htm">Вопросы для самопроверки</a></li-->			
     </ul>
 </li>
 </ul>
  </div><!-- /.navbar-collapse -->
 </div><!-- /.container-fluid -->
</nav>


  <div id="intro" class="section content-section ">
    <div class="container">
       <div class="row">
        <div class="col-lg-12">	


<!-- содержание -->	
<div class="page-header">				
<h3>3.Теоретическая стойкость криптосистем</h3>

<a href="#1" class=punkt>3.1. Введение</a></br>
<a href="#2" class=punkt>3.2. Теория систем с совершенной секретностью</a><br>
<a href="#3" class=punkt>3.3. Шифр Вернама</a><br>
<a href="#4" class=punkt>3.4. Элементы теории информации</a><br>
<a href="#5" class=punkt>3.5. Расстояние единственности шифра с секретным ключом</a><br>
<a href="#6" class=punkt>3.6. Идеальные криптосистемы</a><br>
<a href="#7" class=punkt>Задачи и упражнения</a><br>
</div>
<!--начало-->
<a name=1 class="anchor"></a>
<h3>3.1. Введение</h3>
<p>По-видимому, одна из первых открытых работ по криптографии появилась в 1949 году и принадлежала Шеннону (Claude Shannon, см. [17]). В ней рассматривается классическая схема криптосистемы с секретным ключом, которая была представлена на рис. 1.1.</p>
<p>В этой схеме имеется защищенный канал, предназначенный для передачи секретных ключей. Однако отметим, что в настоящее время можно рассмотреть в качестве защищенного канала схему вычисления секретного ключа на основе методов криптографии с открытым ключом, например, схему Диффи-Хеллмана. В дальнейшем мы будем рассматривать только классическую схему с секретным ключом, но многие результаты переносятся на случай создания секретного канала средствами криптографии с открытым ключом.</p>
<p>Все методы шифрования можно грубо разделить на два больших класса:</p>
<p class=punkt>1)	схемы, принципиально не вскрываемые, что доказывается строго;</p>
<p class=punkt>2)	схемы, стойкость которых основана на том, что дешифрование без ключа, вообще говоря, возможно, но для этого требуется перебор очень большого количества вариантов.</p>
<p>В этой главе мы будем заниматься системами первого типа, стойкость которых теоретически доказана. Системы второго типа будут рассмотрены в следующей главе.</p>
<br><a name=2 class="anchor"></a>
<h3>3.2. Теория систем с совершенной секретностью</h3>
<p>Пусть M = {M<sub>1</sub>, M<sub>2</sub>, M<sub>3</sub>,..., M<sub>m</sub>} — множество всех возможных сообщений например, множество всех текстов длины не более, чем 1000 букв), K = {K<sub>1</sub>, K<sub>2</sub>, K<sub>3</sub>,..., K<sub>n</sub>} — множество всех возможных ключей, E = {E<sub>1</sub>, E<sub>2</sub>,..., E<sub>k</sub>} — множество всех криптограмм (т.е. зашифрованных сообщений). Зашифрованные сообщения зависят от исходного сообщения и ключа, т.е. <img src="img/image301.png" height="27" width="127"> .</p>
<p>Мы будем считать, что на всем множестве сообщений M задано распределение вероятаостей P, т.е. определены вероятности <i>P(M<sub>i</sub>)</i>, i = 1,2,..., m. Это априорное распределение вероятностей, которое известно и противнику. Запись вида <i>P(A\B)</i> будет, как обычно, обозначать условную вероятность события A при условии наступления события B.</p>
<p><b>Определение 3.1.</b> Криптосистема называется совершенно секретной, если выполняется равенство</p>
<p><img src="img/image302.png" height="30" width="193">              (3.1)</p>
<p>при всех <i>M<sub>i</sub></i>, <i>K<sub>l</sub></i> и <img src="img/image301.png" height="27" width="127">.</p>
<p>Поясним это определение. Пусть Ева перехватила криптограмму E<sub>j</sub>. Если (3.1) выполняется для всех возможных сообщений, то это означает, что она не получила никакой информации о переданном сообщении, т.е. знание E<sub>j</sub> совершенно бесполезно для нее. Рассмотрим схематичный пример. Пусть M — множество всех сообщений из шести букв на русском языке. Пусть априори известно, что для нeкоторой системы</p>
<p><img src="img/image303.png" height="48" width="375"></p>
<p>Допустим, мы имеем нeсовeршeнную систему, и Ева после перехвата и вычислений получила следующие данные:</p>
<p><img src="img/image304.png" height="54" width="330"></p>
<p>Это означаeт, что Ева практически расшифровала сообщение: она практически увeрeна, что передано слово «бутыль», так как вероятность, что передано другое сообщение меньше 0.0001.</p>
<p>Если же для рассмотренной системы при любой перехваченной криптограмме E<sub>j</sub> мы получаем</p>
<p><img src="img/image305.png" height="57" width="354"></p>
<p>и такие же равенства выполняются для всех остальных сообщений, то Ева вообще может нe обращать внимание на перехваченный шифротекст E<sub>j</sub>, а, напримeр, отгадывать сообщение на основе исходных вероятаостей. Другими словами, (3.1) — действительно разумное определение совершенно секретной системы.</p>
<p>Исследуем свойства совершенно секретной системы.</p>
<p><b>Теорема 3.1.</b> <i>Если система является совершенно секретной (выполняется (3.1)), то справедливо равенство</i></p>
<p><img src="img/image306.png" height="31" width="176">              (3.2)</p>
<p><i>при всех i и j . Верно и обратное утверждение: если (3.2) выполняется, то система совершенно секретна.</i> </p>
<p>Д о к а з а т е л ь с т в о. По определению условной вероятности</p>
<p><img src="img/image307.png" height="49" width="168"></p>
<p>при <img src="img/image308.png" height="24" width="81"> (см., например, [15]). Поэтому при <img src="img/image309.png" height="24" width="87"> можно записать</p>
<p><img src="img/image310.png" height="49" width="343"></p>
<p>Принимая во внимание (3.1), получаем</p>
<p><img src="img/image311.png" height="46" width="287"></p>
<p>т.е.</p>
<p><img src="img/image312.png" height="52" width="146"></p>
<br><a name=3 class="anchor"></a>
<h3>3.3. Шифр Вернама</h3>
<p>Этот шифр был предложен в 1926 году американским инженером Вернамом (Gilbert Vernam) и использовался на практике, но доказательство его невскрываемости было получено значительно позже Шенноном [17]. Для шифра Вернама часто используется название «одноразовая лента» (one-time pad). Мы опишем этот шифр для случая двоичного алфавита, чтобы упростить обозначения.</p>
<p>Пусть множество сообщений M состоит из слов двоичного алфавита длины n, т.е. всего сообщений не более, чем 2<sup>n</sup>. В шифре Вернама множество ключей также состоит из слов той же длины n и каждый ключ используется с вероятностью 1 /2<sup>n</sup>. Другими словами, все ключи используются с одинаковой вероятностью.</p>
<p>Пусть необходимо зашифровать сообщение <img src="img/image313.png" height="24" width="147"> и пусть выбран ключ <img src="img/image314.png" height="22" width="117">. Тогда зашифрованное сообщению <img src="img/image315.png" height="19" width="116"> получается по формуле:</p>
<p><img src="img/image316.png" height="23" width="120">              (3.3)</p>
<p>где i = 1, 2,..., n и <img src="img/image317.png" height="19" width="22"> обозначает сложение по модулю 2. Другими словами, сообщение шифруется по схеме</p>
<p><img src="img/image318.png" height="75" width="254"></p>
<p>Так как сложение и вычитание по модулю 2 совпадают, то легко видеть, что дешифрование осуществляется по формуле</p>
<p><img src="img/image319.png" height="24" width="133">              (3.4)</p>
<p>П р и м е р 3.1. Пусть <img src="img/image324.png" height="18" width="20">= 01001, <img src="img/image323.png" height="20" width="17"> = 11010. Тогда получаем <img src="img/image322.png" height="17" width="13"> = 10011.  Сложив <img src="img/image322.png" height="17" width="13">  с  <img src="img/image323.png" height="20" width="17">, восстанавливаем  <img src="img/image324.png" height="18" width="20">.</p>
<p><b>Теорема 3.2.</b><i> Шифр Beрнама является совершенно секретной криптосистемой.</i></p>
<p>Д о к а з а т е л ь с т в о. Согласно теореме 3.1 достаточно доказать справедливость (3.2). Имеем</p>
<p><img src="img/image325.png" height="97" width="483"></p>
<p>(в последнем равенстве мы использовали то, что, по предположению, все ключи равновероятны).</p>
<p>Найдем <i>P(E<sub>j</sub>)</i>. По формуле полной вероятности</p>
<p><img src="img/image326.png" height="64" width="256"></p>
<p>Учитывая, что <img src="img/image327.png" height="25" width="133">, получаем </p>
<p><img src="img/image328.png" height="62" width="230"></p>
<p>Так как сумма вероятностей всех возможных сообщений равна 1, получаем</p>
<p><img src="img/image329.png" height="37" width="140"></p>
<p>Таким образом, справедливо (3.2). Теорема доказана.</p>
<p>Известао, что шифр Вернама использовался при защите правительственной связи: например, на так называемой горячей линии «Москва - Вашингтон» [23], а также в других системах, где можно позволить дорогой способ доставки секретного ключа. Однако шифр Вернама можно использовать во многих других практически важных ситуациях. Например, на основе этого шифра легко организовать связь между банком и его филиалами (или связи между банком и клиентами), когда они находятся в одном городе, или защитить электронные письма, скажем, для студентов Алисы и Боба, расстающихся на каникулы (мы предлагаем читателю придумать схему и написать программу, считая, что общая длина писем, которыми они будут обмениваться во время каникул, не превосходит 1.44 Мбайт, т.е. объема стандартной дискеты).</p>
<br><a name=4 class="anchor"></a>
<h3>3.4. Элементы теории информации</h3>
<p>Мы доказали, что шифр Вернама совершенен, однако при его использовании длина ключа равна длине сообщения. К. Шеннон [17] показал, что у любой совершенной системы длина ключа должна быть больше энтропии сообщения (с которой мы кратко познакомимся ниже), т.е. пропорциональна его длине. Во многих практически важных ситуациях приходится использовать короткие ключи (скажем, сотни или тысячи бит) для шифрования длинных сообщений (сотни килобайт и более). В этом случае можно построить только так называемые идеальные системы, впервые описанные Шенноном. Для построения идеальных систем и исследования их свойств Шеннон предложил использовать понятия теории информации. В этом разделе мы определим и кратко проиллюстрируем эти понятия. Достаточно полное и строгое их изложение может быть найдено, например, в [4].</p>
<p>Начнем с определения основного понятия — энтропии Шеннона. Пусть дана дискретная случайная величина <img src="img/image330.png" height="22" width="10">, принимающая значения a<sub>1</sub>, a<sub>2</sub>, . . .a<sub>r</sub> с вероятностями P<sub>1</sub>, P<sub>2</sub>, . . ., P<sub>r</sub>.</p>
<p><b>Определение 3.2.</b> <i>Энтроnия случайной величины <img src="img/image330.png" height="22" width="10"> определяется равенством</i></p>
<p><img src="img/image331.png" height="63" width="200">              (3.5)</p>
<p>где <i>0</i> log <i>0</i> = <i>0</i>.</p>
<p>Если используется двоичный логарифм, то энтропия измеряется в битах, что общепринято в криптографии, теории информации и в компьютерных науках. В случае натуральных логарифмов единица измерения — нат, в случае десятичных — дит.</p>
<p>При r = 2 можно (3.5) записать иначе, введя следующие обозначения: <i>P<sub>1</sub> = p, P<sub>2</sub> = 1 — p.</i> Тогда</p>
<p><img src="img/image332.png" height="34" width="308">              (3.6)</p>
<p>График энтропии для этого случая приведен на рис. 3.1.</p>
<p>Рассмотрим простейшие свойства энтропии.</p>
<p><img src="img/image333.png" height="239" width="320"></p>
<p class=punkt>Рис. 3.1. График двоичной энтропии</p>
<br>
<p><b>Утверждение 3.3.</b></p>
<p><img src="img/image334.png" height="104" width="391"></p>
<p>Д о к а з а т е л ь с т в о. Первое свойство довольно очевидно (см. (3.5)). Второе свойство докажем только для случая r = 2, так как общий случай аналогичен. Исследуем график энтропии. Нужно найти максимум функции (3.6). Для этого мы найдем первую и вторую производные <i>H(p)</i>, считая, что логарифм натуральный.</p>
<p><img src="img/image335.png" height="58" width="544"></p>
<p>Отсюда <i>H'(p) = 0</i> при <i>p = 1/2</i>. Найдем вторую производную</p>
<p><img src="img/image336.png" height="54" width="202"></p>
<p>Мы видим, что <i>H''(p) < 0</i> при <img src="img/image338.png" height="24" width="80">. Это означает, что функция <i>H(p)</i> достигает максимума в точке 1/2 и выпукла на отрезке (0; 1). Таким образом, график, изображенный на рис. 3.1, обоснован. Очевидно, при любом основании логарифма график будет аналогичный.</p>
<p>Для доказательства третьего свойства заметим, что наибольшее значение энтропии для r = 2 равно</p>
<p><img src="img/image339.png" height="34" width="516"></p>
<p>т.е. составляет один бит в случае двоичного логарифма.</p>
<p>«Физический» смысл энтропии состоит в том, что энтропия — это количественная мера неопределенности. В качестве примера рассмотрим три случайные величины для r = 2. Иными словами, будем считать, что у нас есть три источника сообщений, которые порождают буквы a<sub>1</sub>, а<sub>2</sub>, т.е. имеются три случайные величины <img src="img/image330.png" height="22" width="10"> <sub>i</sub>, i = 1,2, 3, принимающие значения a<sub>1</sub> или a<sub>2</sub>:</p>
<p><img src="img/image340.png" height="77" width="303"></p>
<p>Интуитивно ясно, что неопределенность случайной величины <img src="img/image330.png" height="22" width="10"><sub>1</sub> равна нулю. И действительно,</p>
<p><img src="img/image341.png" height="32" width="307"></p>
<p>Посмотрим на <img src="img/image330.png" height="22" width="10"><sub>2</sub> и <img src="img/image330.png" height="22" width="10"><sub>3</sub>. Интуитивно кажется, что неопределенность у <img src="img/image330.png" height="22" width="10"><sub>2</sub> вьше неопределенности у <img src="img/image330.png" height="22" width="10"><sub>3</sub>. Вычислим энтропии:</p>
<p><img src="img/image342.png" height="32" width="135"></p>
<p>(уже считали выше),</p>
<p><img src="img/image343.png" height="29" width="456"></p>
<p>Мы видим, что энтропия действительно является разумной мерой неопределенности. главное, конечно, не примеры такого типа, а то, что эта величина играет ключевую роль во многих задачах теории передачи и хранения информации. В частности, энтропия характеризует максимальную степень сжатия данных. Точнее, если источник сообщений порождает текст достаточно большой длины n с определенной ниже предельной энтропией <i>h</i> на бит сообщения, то этот текст может быть «сжат» в среднем до величины сколь угодно близкой к <i>nh</i>. Например, если <i>h =1/2</i>, то текст сжимается вдвое и т.п. Подчеркнем, что речь идет о так называемом неискажающем сжатии, когда по «сжатому» сообщению можно восстановить исходное.</p>
<p>Рассмотрим теперь двумерную случайную величину, заданную рядом распределения</p>
<p><img src="img/image344.png" height="30" width="429">              (3.7)</p>
<p>Пример 3.2. Из многолетнего опыта преподавания в некотором вузе известно, что оценки за первый и второй контрольный срок по математике (соответственно <img src="img/image330.png" height="22" width="10"><sub>1</sub> и <img src="img/image330.png" height="22" width="10"><sub>2</sub>) подчиняются закону распределения, задаваемому табл. 3.1.</p><br>
<p>Таблица 3.1. Распределение оценок за контрольный срок</p>
<p><img src="img/image345.png" height="106" width="357"></p><br>
<p>Введем следующие обозначения:</p>
<p><img src="img/image346.png" height="95" width="260"></p>
<p>Напомним некоторые элементарные соотношения, известные из теории вероятностей (см. [15]):</p>
<p><img src="img/image347.png" height="60" width="189">              (3.8)</p>
<p><img src="img/image348.png" height="34" width="357">              (3.9)</p>
<p><img src="img/image349.png" height="44" width="272">              (3.10)</p>
<p>((3.10) — уже встречавшаяся формула полной вероятности, в которой <i>H<sub>i</sub></i> — попарно несовместные события, сумма которых содержит событие A ).</p>
<p>В соответствии с (3.5) определим энтропию двумерной случайной величины</p>
<p><img src="img/image350.png" height="59" width="293">              (3.11)</p>
<p>Aналогично для трехмерной случайной величины (<img src="img/image330.png" height="22" width="10"><sub>1</sub>, <img src="img/image330.png" height="22" width="10"><sub>2</sub>, <img src="img/image330.png" height="22" width="10"><sub>3</sub>) и распределения вероятностей <i>P<sub>ijk</sub></i> определим</p>
<p><img src="img/image351.png" height="55" width="338">              (3.12)</p>
<p>Подобным же образом определяется энтропия для n-мерной случайной величины.</p>
<p>Представим теперь, что значение случайной величины <img src="img/image330.png" height="22" width="10"><sub>1</sub> известно, а <img src="img/image330.png" height="22" width="10"><sub>2</sub> — неизвестно. Тогда естественно определить условную энтропию</p>
<p><img src="img/image352.png" height="61" width="307">              (3.13)</p>
<p>Это — средняя условная энтропия случайной величины <img src="img/image330.png" height="22" width="10"><sub>2</sub> при условии, что значение <img src="img/image330.png" height="22" width="10"><sub>1</sub> известно.</p>
<p><b>Утверждение 3.4 (свойство двумерной энтропии)</b>.</p>
<p><img src="img/image353.png" height="38" width="266">              (3.14)</p>
<p>в частности, для независимыx случайных величин <img src="img/image330.png" height="22" width="10"><sub>1</sub> и <img src="img/image330.png" height="22" width="10"><sub>2</sub></p>
<p><img src="img/image354.png" height="53" width="262">              (3.15)</p>
<p>Напомним, что <img src="img/image330.png" height="22" width="10"><sub>1</sub>, <img src="img/image330.png" height="22" width="10"><sub>2</sub> независимы, если <i>P<sub>ij</sub> = P<sub>i</sub>P<sub>j</sub></i> для всех <i>i</i> и <i>j</i>. Доказательство утверждения достаточно просто и может быть найдено в [4]. Мы ограничимся только его интерпретацией. Пусть в первом опыте порождается <img src="img/image330.png" height="22" width="10"><sub>2</sub>, во втором — <img src="img/image330.png" height="22" width="10"><sub>1</sub>. Тогда общая неопределенность экперимента должна быть равна неопределенности первого опыта, сложенной с условной неопределенностью второго опыта. В случае независимых <img src="img/image330.png" height="22" width="10"><sub>1</sub> и <img src="img/image330.png" height="22" width="10"><sub>2</sub> знание одной величины не несет никакой информации о другой, что соответствует (3.15).</p>
<p>Пусть дана n-мерная случайная величина (<img src="img/image330.png" height="22" width="10"><sub>1</sub>, <img src="img/image330.png" height="22" width="10"><sub>2</sub>,... ,<img src="img/image330.png" height="22" width="10"><sub>n</sub>). Справедливо следующее соотношение [4]:</p>
<p><img src="img/image355.png" height="25" width="554">              (3.16)</p>
<p>Для независимых случайных величин</p>
<p><img src="img/image356.png" height="59" width="291">              (3.17)</p>
<p>(заметим, что (3.14), (3.15) — частный случай (3.16), (3.17)). </p>
<p>В общем случае</p>
<p><img src="img/image357.png" height="38" width="263">              (3.18)</p>
<p>Рассмотрим последовательность случайных величин <img src="img/image330.png" height="22" width="10"><sub>1</sub>, <img src="img/image330.png" height="22" width="10"><sub>2</sub>, <img src="img/image330.png" height="22" width="10"><sub>3</sub>,. . . (<img src="img/image330.png" height="22" width="10"><sub>i</sub> принимают значения в A), которую можно рассматривать как случайный процесс с дискретным временем. Мы будем считать, что этот процесс стационарный, т.е., неформально, вероятностные характеристики для (<img src="img/image330.png" height="22" width="10"><sub>1</sub>. . . <img src="img/image330.png" height="22" width="10"><sub>n</sub>) те же, что и для (<img src="img/image330.png" height="22" width="10"><sub><img src="img/image358.png" height="16" width="17">+1</sub>. . . <img src="img/image330.png" height="22" width="10"><sub><img src="img/image358.png" height="16" width="17">+n</sub>) при всех положительных n и <img src="img/image358.png" height="16" width="17"> (точное определение дано в [4]).</p>
<p>Пусть <i>H(<img src="img/image330.png" height="22" width="10"><sub>1</sub>,..., <img src="img/image330.png" height="22" width="10"><sub>n</sub>)</i> — энтропия n-мерной случайной величины (<img src="img/image330.png" height="22" width="10"><sub>1</sub>... <img src="img/image330.png" height="22" width="10"><sub>n</sub>). Обозначим через</p>
<p><img src="img/image359.png" height="49" width="205"></p>
<p>удельную энтропию n-го порядка и определим</p>
<p><img src="img/image360.png" height="37" width="223"></p>
<p>Отметим следующие свойства:</p>
<p><img src="img/image361.png" height="27" width="198">              (3.19)</p>
<p><img src="img/image362.png" height="24" width="181">              (3.20)</p>
<p><img src="img/image363.png" height="27" width="191">              (3.21)</p>
<p>Их доказательство может быть найдено в [4].</p>
<p>Для независимых <img src="img/image330.png" height="22" width="10"><sub>1</sub>, <img src="img/image330.png" height="22" width="10"><sub>2</sub>,..., <img src="img/image330.png" height="22" width="10"><sub>n</sub> справедливы равенства</p>
<p><img src="img/image364.png" height="33" width="178"></p>
<p>(Процесс, порождающий независимые случайные величины называется процессом без памяти.)</p>
<p><b>Теорема 3.5.</b><i> Для стационарного процесса существуют пределы <img src="img/image365.png" height="26" width="192">, причем эти пределы равны</i>.</p>
<p>Доказательство теоремы см. в [4].</p>
<p>Обозначим общее значение этих пределов через <img src="img/image366.png" height="21" width="28">,</p>
<p><img src="img/image367.png" height="33" width="225">              (3.22)</p>
<p>Пусть дан алфавит A = (a<sub>1</sub>, a<sub>2</sub>,... a<sub>r</sub>). Мы знаем, что</p>
<p><img src="img/image368.png" height="34" width="178"></p>
<p>для процесса без памяти, поэтому, принимая во внимание (3.21) и (3.22), получаем <img src="img/image369.png" height="19" width="125">, причем максимум достигается для процессов без памяти, у которых все буквы порождаются с равными вероятностями <i>1/r</i>. Естественно ввести величину</p>
<p><img src="img/image370.png" height="29" width="155">              (3.23)</p>
<p>называемую избыточностью (на букву сообщения). Неформально, это как бы неиспользованная часть алфавита. Избыточность — количественная мера взаимной зависимости символов и их «неравновероятности». Отметим, что в примере из первой главы во втором случае, когда даже простой шифр Цезаря не вскрываем, избыточность шифруемого сообщения равна нулю, так как все десять символов независимы и равновероятны, т.е. <img src="img/image371.png" height="18" width="98"> и R = 0.</p>
<br>
<a name=5 class="anchor"></a>
<h3>3.5. Расстояние единственности шифра с секретным ключом</h3>
<p>Рассмотрим криптосистему с секретаым ключом, схема которой показана на рис. 1.1. Пусть источник порождает сообщение 
<img src="img/image313.png" height="24" width="147">. напримeр, <i>m<sub>i</sub></i> при каждом <i>i</i> — это буква из русского алфавита или знак пробела, а <img src="img/image372.png" height="22" width="21"> — сообщение на русском языке. Алиса и Боб обладают секретным ключом <i>k</i>, известным только им, и пусть <img src="img/image315.png" height="19" width="116"> — сообщение, зашифрованное при помощи этого ключа.</p>
<p>П р и м е р 3.3. Пусть источник порождает буквы из алфавита <i>A = {a,b,c}</i> с вероятаостями </i>P(a) = 0.8, P(b) = 0.15, P(c) = 0.05</i>, и пусть это источник без памяти. Пусть шифратор, применяя ключ k, заменяет буквы в исходном сообщении, используя какую-либо перестановку символов:</p>
<p><img src="img/image373.png" height="142" width="185"></p>
<p>т.е. ключ принимает значения от 1 до 6 , и если , напримeр, <i> k = 5</i> , то в исходном тексте осуществляется следующая замена символов: <i>a <img src="img/image374.png" height="11" width="22"> c, b <img src="img/image374.png" height="11" width="22"> a, c <img src="img/image374.png" height="11" width="22"> b</i>.</p>
<p>Пусть Ева перехватила зашифрованное сообщение</p>
<p><img src="img/image375.png" height="24" width="98"></p>
<p>и хочет определить значение ключа. Оценим количественно вероятности использования всех возможных ключей, используя формулу Байеса</p>
<p><img src="img/image376.png" height="60" width="293"></p>
<p>где <i>E, K<sub>1</sub>, ..., K<sub>t</sub></i> — некоторые события, причем <i>K<sub>i</sub></i> попарно нeсовместны и <img src="img/image377.png" height="24" width="105">. В нашем случае событие <i>E</i> — это получение зашифрованного сообщения<img src="img/image375.png" height="20" width="94">,<i> t = 6</i>, а <i>K<sub>i</sub></i> означает, что выбран ключ <i>k = i</i>.</p>
<p>Мы предполагаем, что все ключи равновероятны, т.е.</p>
<p><img src="img/image378.png" height="25" width="509"></p>
<p>Тогда</p>
<p><img src="img/image379.png" height="170" width="461"></p>
<p>Отсюда легко находим</p>
<p><img src="img/image380.png" height="69" width="291"></p>
<p>и получаем по формуле Байеса апостериорную вероятность того, что был использован ключ k = 1, при условии, что получено сообщение<img src="img/image375.png" height="24" width="98"></p>
<p><img src="img/image381.png" height="40" width="534"></p>
<p>Продолжая аналогично, находим, что наиболее вероятны ключи k = 5 и k = 6:</p>
<p><img src="img/image382.png" height="62" width="365"></p>
<p>а вероятности всех остальных ключей меньше 0.01.</p>
<p>Мы видим, что, перехватив всего 5 букв, Ева может определить ключ почти однозначно. Таким образом, из этого примера и из примера с шифром Цезаря в первой главе мы заключаем, что, по-видимому, существует некоторая длина перехваченного сообщения, после которой ключ может быть найден с вероятностью, близкой к единице.</p>
<p><b>Утверждение 3.6 (о расстоянии единственности шифра (Шеннон [17])).</b> <i>Пусть рассматривается система с секретным ключом, и пусть <i>H(K)</i> — энтропия ключа. Пусть R — избыточность шифруемого сообщения, а n — длина сообщения, такая, что</i></p>
<p><img src="img/image383.png" height="34" width="205">              (3.24)</p>
<p><i>т.е. при этой длине перехваченного сообщения ключ почти однозначно восстановлет. Тогда справедливо нeравeнство</i></p>
<p><img src="img/image384.png" height="50" width="117">              (3.25)</p>
<p>Дадим несколько замечаний к этому утверждению.</p>
<p>1.	Число n, удовлетворяющее неравенству (3.25), называется расстоянием единственности шифра. Это означает, что в среднем достаточно перехватить n букв зашифрованного сообщения для восстановления ключа.
<p>2.	Мы видим, что если избыточность сообщения R = 0, то ключ никогда не будет определен так как n = х .Т.е. шифр невозможно взломать (мы видели это в примере с номером замка из первой главы).
<p>3.	Уменьшение избыточности может быть достигнуто за счет сжатия данных. Дело в том, что при сжатии данных энтропия «сжатого» текста сохраняется, а длина уменьшается. Следовательно, энтропия на букву в сжатом тексте больше, чем в исходном, а избыточность меньше, см. (3.23). 3начит, после сжимающего кодирования расстояние единственности шифра увеличивается.
<p>4.	С практической точки зрения лучше использовать системы, в которых ключ меняется задолго до достижения расстояния единственности шифра.
</p>
<p>Д о к а з а т е л ь с т в о. Мы дадим здесь только основную идею доказательства. Пусть противник, перехватив переданный шифротекст <img src="img/image315.png" height="19" width="116">, однозначно восстановил ключ, а тем самым и исходное сообщение. 3начит, неопределенность противника уменьшилась на <i>H(K) + H (m<sub>1</sub>,..., m<sub>n</sub>)</i>, так как он узнал и ключ, и исходное сообщение. При этом он получил n букв из <i>r</i>-буквенного алфавита <i>{a<sub>1</sub>,... ,a<sub>r</sub>}</i>. Мы знаем, что максимальное значение энтропии <img src="img/image385.png" height="20" width="97">, значит, неопределенность противника не может уменьшаться больше, чем на <i>n log r</i>. Отсюда</p>
<p><img src="img/image386.png" height="27" width="290"></p>
<p>следовательно,</p>
<p><img src="img/image387.png" height="27" width="332"></p>
<p>откуда получаем, что</p>
<p><img src="img/image388.png" height="46" width="340"></p>
<p>(здесь мы воспользовались тем, что <img src="img/image389.png" height="23" width="198">, и определением избыточности (3.23)).</p>
<p>П р и м е р 3.4. Оценим расстояние единственности для шифра из примера 3.3. Имеем <img src="img/image390.png" height="22" width="340">, и энтропия на букву источника равна</p>
<p><img src="img/image391.png" height="29" width="446"></p>
<p>Поэтому</p>
<p><img src="img/image392.png" height="48" width="200"></p>
<p>Мы видели, что пяти букв было практически достаточно для раскрытия ключа. И нeравeнство (3.25) хорошо согласуется с нашим примером.</p>
<p>Поясним еще на одном примере, как взаимная зависимость символов увеличивает избыточность и тем самым уменьшает расстояние единственности.</p>
<p>П р и м е р 3.5. Пусть дан марковский источник, т.е. источник с памятью, в котором вероятность появления очередного символа зависит только от предыдущего символа. Источник описывается следующей матрицей переходов:</p>
<p><img src="img/image393.png" height="84" width="152"></p>
<p>и начальными вероятностями <i>P(a) = 0.19</i>, <i>P(b) = 0.34</i>, <i>P(c) = 0.47</i>. Пусть, как и в примере 3.3, используется шифр с шестью возможными ключами, и пусть пeрeхвачeн зашифрованный текст</p>
<p><img src="img/image394.png" height="27" width="115">.</p>
<p>Мы видим по матрице переходов, что сочетание <i>aa</i> нeвозможно (после буквы <i>a</i> вероятность появления снова буквы <i>a</i> равна нулю), а сочетание <i>bb</i> — маловероятно (вероятность появления <i>b</i> после <i>b</i> равна 0.1). Следовательно, скорее всего первая пара букв соответствует буквам <i>cc</i> исходного сообщения, т.е. при шифровании была использована подстановка <i>c</i><img src="img/image374.png" height="11" width="22"> <i>b</i>. Тогда <i>ac</i> соответствует исходным парам <i>ab</i>
или <i>ba</i>. По матрице мы видим, что сочетание <i>ba</i> невозможно, а возможно только <i>ab</i>. Поэтому мы можем предположить, что в качестве ключа была использована перестановка номер 2:</p>
<p><img src="img/image395.png" height="32" width="262"></p>
<p>Вычислим точные вероятности использования различных ключей, как в примере 3.3. Заметим, что вероятность конкретного сообщения источника равна произведению вероятности начальной буквы и вероятностей переходов от одной буквы к другой.</p>
<p><img src="img/image396.png" height="220" width="538"></p>
<p>Отсюда находим</p>
<p><img src="img/image397.png" height="67" width="294"></p>
<p>и получаем по формуле Бейеса апостериорные вероятности использованных ключей при условии, что получено сообщение <img src="img/image394.png" height="27" width="115">: </p>
<p><img src="img/image398.png" height="160" width="400"></p>
<p>Эти вычисления подтверждают справедливость приведенного ранее неформального рассуждения.</p>
<p>Оценку расстояния единственности шифра можно использовать при конструировании криптосистем. Например, кажется разумным менять ключ на новый, когда общая длина зашифрованных с его помощью сообщений приближается к величине расстояния единственности.</p>
<p>Новые подходы к построению теоретически стойких криптосистем, связанные с применением методов специального кодирования, изложены в работах [10, 13, 14, 16, 24, 25] авторов данной книги. Предлагаемые там методы достаточно сложны для описания, однако они эффективны с вычислительной точки зрения и позволяют строить невскрываемые шифры с секретаым ключом. Основная идея этих методов состоит в обеспечении путем кодирования нулевой избыточности сообщения, которое подлежит шифрованию. Один из таких методов будет рассмотрен в следующем разделе.</p>
<br>
<a name=6 class="anchor"></a>
<h3>3.6. Идеальные криптосистемы</h3>
<p>В разд. 3.2 было введено понятие совершенной секретности, а затем было показано, что шифр Вернама совершенно секретен. Мы видели, что в этом шифре длина ключа равна длине сообщения и ключ используется всего один раз. Если же мы хотим использовать короткий многоразовый ключ (что актуально для большинства практических приложений), то какой наилучший результат в смысле стойкости шифра мы можем достичь? При обсуждении утверждения 3.6 указывалось, что при нулевой избыточности сообщения расстояние единственности шифра бесконечно. Это означает, что даже короткий (или, что эквивалентно, применяемый много раз) ключ, используемый для шифрования очень длинного сообщения нулевой избыточности, не может быть раскрыт. А это, в свою очередь, означает, что у противника, пытающегося разгадать зашифрованный текст, остается неопределенность, равная неопределенности ключа. Очевидно, это лучшее, что может быть достигнуто в данных условиях (здесь можно снова вспомнить пример с кодовым замком из первой главы). Эти рассуждения подводят нас к понятию строго идеального шифра, впервые введенному Шенноном [17].</p>
<p>Пусть сообщение <i>m<sub>1</sub>m<sub>2</sub> ... m<sub>t</sub></i> шифруется при помощи секретного ключа <img src="img/image400.png" height="21" width="122">, в результате чего получается зашифрованное сообщение <img src="img/image399.png" height="19" width="115">. Обозначим через <i>H(m<sub>1</sub>m<sub>2</sub> ... m<sub>t</sub>)</i> энтропию сообщения, через <img src="img/image401.png" height="24" width="38"> и <img src="img/image402.png" height="20" width="38"> — соответственно энтропии шифротекста и ключа. Тогда <img src="img/image403.png" height="20" width="140"> представляет неопределенность сообщения, а <img src="img/image404.png" height="26" width="56"> — неопределенность ключа при условии, что известен шифротекст <img src="img/image405.png" height="18" width="16">.</p>
<p><b>Определение 3.3.</b> Шифр называется <i>строго идеальным</i>, если</p>
<p><img src="img/image406.png" height="27" width="473">              (3.26)</p>
<p>Если энтропия ключа меньше энтропии сообщения источника, то (3.26) упрощается:</p>
<p><img src="img/image407.png" height="26" width="306">              (3.27)</p>
<p>при всех достаточно больших t. Так как мы будем рассматривать случай, когда длина сообщения t велика, то в качестве определения строго идеального шифра будем использовать (3.27) .</p>
<p>Неформально, строгая идеальность шифра означает, что количество решений криптограммы равно количеству различных ключей и все решения равновероятны, как в примере с кодовым замком.</p>
<p>В этом разделе мы рассмотрим конструкцию идеальной системы, недавно предложенную в [10], ограничившись описанием только основной идеи применительно к случаю, когда сообщение порождается двоичным источником без памяти с неизвестной статистикой, иными словами, делается последовательно и независимо случайный выбор буквы из алфавита <i>A = {a<sub>1</sub>,a<sub>2</sub>}</i>, причем вероятности выбора букв неизвестны.</p>
<p>Пусть источник порождает потенциально неограниченные сообщения <i>m<sub>1</sub>m<sub>2</sub> ...m<sub>t</sub></i>, <img src="img/image408.png" height="19" width="71">, и имеется ключ фиксированной длины<img src="img/image400.png" height="21" width="122">, <img src="img/image409.png" height="19" width="54">. (Как мы упомянули вьше, предполагается также, что энтропия источника на букву отлична от нуля, так как в противном случае вообще нет необходимости в передаче сообщений.) Будем последовательно разбивать сообщение источника на блоки символов длины n, где n > 1 — параметр метода. Обозначим один из таких блоков через <img src="img/image410.png" height="17" width="18">. Опишем преобразования, выполняемые над каждым таким n-буквенным блоком.</p>
<p>Вначале определим количество букв <i>а<sub>1</sub></i> и<i> а<sub>2</sub></i> в блоке <img src="img/image410.png" height="17" width="18">. Пусть, скажем, имеется <i>n<sub>1</sub></i> букв <i>а<sub>1</sub></i> и <i>n<sub>2</sub></i> = <i>n - n<sub>1</sub></i> букв <i> а<sub>2</sub></i>. Определим <img src="img/image411.png" height="18" width="43"> как слово длины <img src="img/image412.png" height="24" width="96"> бит, кодирующее <i>n<sub>1</sub></i>.</p>
<p>Теперь рассмотрим множество <i>S</i> всех последовательностей, состоящих из <i>n<sub>1</sub></i> букв <i>a<sub>1</sub></i> и <i>n<sub>2</sub></i> букв <i>a<sub>2</sub></i>. В этом множестве </p>
<p><img src="img/image413.png" height="50" width="233"></p>
<p>элементов. Несмотря на то, что нам не известны вероятности последовательностей из множества <i>S</i>, одно мы можем сказать точно — все они равны между собой (в силу независимости выбора отдельных букв сообщения). Зададим на множестве <i>S</i> некоторый порядок, например, расположим сообщения в порядке возрастания соответствующих им двоичных чисел (считая, что <i>а<sub>1</sub> = 0</i>, <i>а<sub>2</sub> = 1</i>). Вычислим номер данного конкретного блока <img src="img/image410.png" height="17" width="18"> вутри упорядоченного множества <i>S</i> (для вычисления такого номера известен эффективный алгоритм [11], описание которого выходит за рамки нашей книги). Обозначим этот номер через <img src="img/image414.png" height="21" width="47">.</p>
<p>Разобьем множество <i>S</i> на непересекающиеся подмножества <i>S<sub>0</sub>, S<sub>1</sub>, ..., S<sub>v</sub></i> с числами элементов, равными различным степеням двойки (например, если <i>|S| = 21</i>, то получаем три подмножества с числами элементов 16, 4 и 1). По <img src="img/image414.png" height="21" width="47"> определим, какому подмножеству принадлежит <img src="img/image410.png" height="17" width="18"> (обозначим номер такого подмножества через <img src="img/image415.png" height="23" width="42">), и найдем номер m внутри данного подмножества (обозначим этот номер через <img src="img/image414.png" height="21" width="47">).</p>
<p>Посмотрим внимательно на номер сообщения внутри подмножества, <img src="img/image414.png" height="21" width="47">. Замечательно то, что <img src="img/image414.png" height="21" width="47"> — это полностью случайная последовательность нулей и единиц (т.е. такая, где символы независимы, а вероятаости нуля и единицы равны). Действительно, <img src="img/image414.png" height="21" width="47"> — это номер одной из равновероятных последовательностей букв в подмножестве из <i>2<sup>b</sup></i> элементов (для некоторого <i>b</i>). Номера всех таких последовательностей — это всевозможные последовательности из <i>b</i> двоичных цифр. если все последовательности из <i>b</i> двоичных цифр равновероятны, то отдельные символы равновероятны и независимы.</p>
<p>Итак, обрабатывая описанным образом последовательные блоки сообщения, мы представляем сообщение в виде</p>
<p><img src="img/image416.png" height="30" width="356"></p>
<p>Теперь перейдем к описанию процесса шифрования преобразованного сообщения. На первый взгляд это может показаться странным, но слова <i>u(<span style='font-size:8.0pt;line-height:150%'>•</span>)</i> и <i>v(<span style='font-size:8.0pt;line-height:150%'>•</span>)</i> вообще не шифруются! Шифруются только слова <img src="img/image419.png" height="22" width="40"> с использованием секретного ключа <sup><img src="img/image420.png" height="19" width="15"></sup>. В качестве Шифра можно, Например, использовать побитовое сложение по модулю 2 с периодически продолженным ключом. Для описания этого шифра удобно занумeровать символы слов <img src="img/image419.png" height="22" width="40"> подряд и обозначить их через <img src="img/image418.png" height="16" width="90">. Тогда шифрование проводится по формуле</p>
<p><img src="img/image421.png" height="22" width="171"></p>
<p>В результате примeнeния описанного метода мы зашифровали сообщение следующим образом:</p>
<p><img src="img/image422.png" height="25" width="492">              (3.28)</p>
<p>По построению алгоритма ясно, что из правой части (3.28) можно восстановить сообщение, если знать <sup><img src="img/image420.png" height="19" width="15"></sup> . Вначалe нужно дешифровать символы слов <img src="img/image419.png" height="22" width="40">, используя формулу</p>
<p><img src="img/image423.png" height="24" width="167">              (3.29)</p>
<p>а затем из слов <img src="img/image424.png" height="19" width="93"> восстанавливаются последовательные блоки сообщения.</p>
<p>П р и м е р 3.6. Пусть требуется зашифровать сообщение</p>
<p><img src="img/image425.png" height="22" width="198"></p>
<p>с трехбитовым ключом <sup><img src="img/image420.png" height="19" width="15"></sup> = 011. Разобьем сообщение на два блока по пять символов, <i>n = 5</i>.</p>
<p>Выполним преобразование для первого блока <img src="img/image426.png" height="18" width="140">. Для этого блока <i>n<sub>1</sub> = 1</i> и <img src="img/image427.png" height="23" width="128">. Рассмотрим теперь упорядоченное множество всех сообщений, состоящих из одной буквы <i>a<sub>1</sub></i> и четырех букв <i>a<sub>2</sub></i> (табл. 3.2). Всего таких сообщений <img src="img/image428.png" height="24" width="68">, поэтому имеем два подмножества <i>S<sub>0</sub></i> и <i>S<sub>1</sub></i> с числом элементов соответственно 4 и 1. Мы видим, что <img src="img/image431.png" height="20" width="30"> входит в <i>S<sub>0</sub></i> под номером <i>2 = (10)<sub>2</sub></i>. Таким образом, мы получаем следующие два слова: <img src="img/image429.png" height="19" width="52"> = 0, <img src="img/image430.png" height="20" width="56"> = (10)<sub>2</sub>.</p>
<p>Таблица 3.2. Множество
равновероятных сообщений; <i>n<sub>1</sub> = 1, n<sub>2</sub> = 4</i>
</p><p><img src="img/image432.png" height="168" width="497"></p><br>
<p>Теперь выполним преобразование для второго блока сообщения <img src="img/image433.png" height="19" width="134">. Для этого блока <i>n<sub>1</sub> = 2</i> и <img src="img/image434.png" height="24" width="128">. Рассмотрим упорядоченное множество всех сообщений, состоящих из двух букв <i>а<sub>1</sub></i> и трех букв <i>а<sub>2</sub></i> (табл. 3.3). Всего таких сообщений <img src="img/image435.png" height="28" width="29"> = 10, поэтому имеем два подмножества <i>S<sub>0</sub></i> и <i>S<sub>1</sub></i> с числом элементов соответственно 8 и 2. Мы видим, что <img src="img/image436.png" height="20" width="25"> входит под номером 6 = (110)<sub>2</sub> в <i>S<sub>0</sub></i>. Таким образом, мы получаем <img src="img/image437.png" height="24" width="222">.</p>
<p>В результате мы получили следующий двоичный код преобразованного сообщения:</p>
<p class=punkt>001 0 10 010 0 110</p>
<p>(пробелы здесь поставлены только для удобства чтения; для однозначного декодирования они не нужны).</p>
<p>Теперь зашифруем преобразованное сообщение. Периодически продолженный ключ имеет вид <img src="img/image438.png" height="20" width="119">. Сложение битов слов <img src="img/image419.png" height="22" width="40"> с этим ключом дает</p>
<p><img src="img/image439.png" height="82" width="182"></p>
<p>Таблица 3.3. Множество
равновероятных сообщений; <i>n<sub>1</sub> = 2, n<sub>2</sub> = 3</i>
</p><p><img src="img/image440.png" height="311" width="472"></p><br>
<p>Зашифрованное сообщение выглядит следующим образом:</p>
<p class=punkt><img src="img/image441.png" height="18" width="20">= 001 0 11 010 0 011.</p>
<p>Остановимся теперь на основных свойствах рассмотренного метода.</p>
<p><b>Утверждение 3.7.</b><i> Построенный шифр строго идеален.</i></p>
<p>Д о к а з а т е л ь с т в о. Как уже отмечалось при изложении метода, слово <img src="img/image414.png" height="21" width="47"> для каждого блока <img src="img/image410.png" height="17" width="18"> состоит из равновероятных и независимых символов 0 и 1, другими словами, «полностью» случайно. Так как блоки в сообщении независимы, то в преобразованном сообщении последовательность слов <img src="img/image442.png" height="20" width="249">. также полностью случайна. Но любая последовательность <img src="img/image443.png" height="17" width="90"> соответствует некоторому сообщению, и все такие сообщения равновероятны. Поэтому при подстановке любого ключа в дешифрующее выражение (3.29) мы получаем какое-либо решение, причем все решения равновероятны. В результате, имея только шифротекст <img src="img/image441.png" height="18" width="20">, мы ничего не можем сказать об использованном ключе, т.е.</p>
<p><img src="img/image444.png" height="30" width="130"></p>
<p>Далее, каждому конкретному сообщению <i>m<sub>1</sub>m<sub>2</sub> ... m<sub>t</sub></i> соответствует одна и только одна последовательность <img src="img/image443.png" height="17" width="90">, и при достаточно большом t, а именно, таком, что длина последовательности <img src="img/image443.png" height="17" width="90"> не меньше длины ключа, каждому ключу, подставляемому в (3.29), соответствуют различные равновероятные сообщения. Поэтому</p>
<p><img src="img/image445.png" height="27" width="237"></p>
<p>Ненулевая энтропия источника гарантирует то, что требуемое достаточно большое t всегда существует.</p>
<p>Таким образом, мы доказали, что (3.27) выполняется.</p>
<p>Особенностью предложенного шифра является то, что шифрованию подвергается не все преобразованное сообщение, а только его часть. В приведенном примере даже может показаться, что слишком много информации остается «открытой». Какая все-таки доля информации скрывается этим шифром? Следующее утверждение дает ответ на этот вопрос.</p>
<p><b>Утверждение 3.8.</b> <i>Пусть сообщение порождается источником без памяти с энтропией h на букву . Тогда для каждого блока <img src="img/image410.png" height="17" width="18"> из n символов сообщения средняя длина шифруемого слова <img src="img/image414.png" height="21" width="47"> удовлетворяет неравенству</i></p>
<p><img src="img/image446.png" height="28" width="264">              (3.30)</p>
<p>(здесь <i>E(<span style='font-size:8.0pt;line-height:150%'>•</span>)</i> —математическое ожидание, <i>а</i> | <span style='font-size:8.0pt;line-height:150%'>•</span> |— длина).</p>
<p>Д о к а з а т е л ь с т в о. Компонента кода <img src="img/image411.png" height="18" width="43"> может принимать любое значение от 0 до n, и поэтому ее максимальная энтропия равна<i> log(n + 1)</i>.</p>
<p>Слово <img src="img/image415.png" height="23" width="42"> может принимать любое значение от 0 до <img src="img/image447.png" height="13" width="16">, что связано с разбиением <i>S</i> на подмножества <i>S<sub>0</sub>, S<sub>1</sub>, ..., S<sub>v</sub></i>. Очевидно, что <img src="img/image448.png" height="23" width="110">. Из известаого неравенства <img src="img/image449.png" height="24" width="137"> получаем <img src="img/image450.png" height="23" width="231">. Поэтому максимальная энтропия слова <img src="img/image415.png" height="23" width="42"> строго меньше <i>log(n + 1)</i>.</p>
<p>Энтропия блока равна <img src="img/image451.png" height="21" width="94"> (так как символы порождаются источником без памяти). При преобразовании блока энтропия не изменяется, поэтому для энтропии слова <img src="img/image414.png" height="21" width="47"> имеем</p>
<p><img src="img/image452.png" height="31" width="250"></p>
<p>(из общей энтропии мы вычли верхнюю границу максимальной энтропии слов <img src="img/image411.png" height="18" width="43"> и <img src="img/image415.png" height="23" width="42">). так как слово <img src="img/image414.png" height="21" width="47"> состоит из равновероятных и независимых символов 0 и 1, его средняя длина совпадает с энтропией, что завершает доказательство.</p>
<p>Неформально, утверждение 3.8 говорит о том, что «почти вся» информация сообщения содержится в шифруемой компоненте кода <img src="img/image453.png" height="13" width="21">, если длина блока n достаточно велика. Иными словами, представленный шифр скрывает «почти всю» информацию. Причем даже полный перебор ключей не позволяет вскрыть шифр.</p>
<p>Конечно, рассмотренная конструкция идеальной криптосистемы может иметь различные варианты построения. Например, может представлять интерес вариант, в котором часть ключа используется для «закрытия» префикса (т.е. компонент <i>u</i> и <i>v</i> кода). Правда, в этом случае система будет «просто» идеальной (не строго идеальной). </p>
<br><a name=7 class="anchor"></a>
<h3>Задачи и упражнения</h3>
<p><b>3.1.</b>	Зашифровать сообщение <img src="img/image410.png" height="17" width="18"> шифром Вернама с ключом <sup><img src="img/image420.png" height="19" width="15"></sup>:</p>
<p><img src="img/image454.png" height="141" width="339"></p>
<p><a href="lec6.htm#3.1" class=punkt>См. ответ.</a></p>
<p><b>3.2.</b>	Пусть источник без памяти порождает буквы из алфавита <i>A = {a, b, c}</i> с вероятаостями <i>P(a), P(b), P(c)</i>. Шифратор заменяет буквы, используя одну из шести возможных перестановок, как это делалось в примере 3.3. Определить апостериорные вероятности использованных ключей для заданного зашифрованного сообщения <img src="img/image441.png" height="18" width="20">:</p>
<p><img src="img/image455.png" height="150" width="462"></p>
<p><a href="lec6.htm#3.2" class=punkt>См. ответ.</a></p>
<p><b>3.3.</b>	Для источников задачи 3.2 вычислить энтропию и расстояние единственности. </p>
<p><a href="lec6.htm#3.3" class=punkt>См. ответ.</a></p>
<p><b>3.4.</b>	По имеющемуся зашифрованному сообщению <img src="img/image441.png" height="18" width="20"> найти апостериорные вероятности использованных ключей и соответствующие им сообщения, если известно, что используется шифр примера 3.3, а сообщения порождаются марковским источником, описанным в примере 3.5:</p>
<p><img src="img/image456.png" height="150" width="189"></p>
<p><a href="lec6.htm#3.4" class=punkt>См. ответ.</a></p>






<br><br>
<!--конец-->
      
				

				</div>
			</div>
			
      </div>
    </div>
  </div>
	
	<!--Меню навигации по Темам -->	
	<div aria-label="..." class = "nav-menu">
	 <ul class="pager background-transition-slow">
		 <li title="Наверх" style = "margin-right:15px;	"><a class = "glyphicon glyphicon-menu-up page-scroll" href="#page-top"></a></li>
		 <li title="К предыдущей лекции"><a class = "glyphicon glyphicon-menu-left page-scroll" href="lec2.htm"></a></li>
		 <li title="В содержание"><a class = "glyphicon glyphicon-list-alt" href="lec_index.htm"></a></li>
		 <li title="К следующей лекции"><a class = "glyphicon glyphicon-menu-right page-scroll" href="lec4.htm"></a></li>
	 </ul>
	</div>
	<!---->
	
	</div>
  <!-- jQuery -->
  <script src="../lib/js/jquery.js"></script>

  <!-- Bootstrap Core JavaScript -->
  <script src="../lib/js/bootstrap.min.js"></script>

  <!-- Scrolling Nav JavaScript -->
  <script src="../lib/js/jquery.easing.min.js"></script>
  <script src="../lib/js/scrolling-nav.js"></script>

</body>

</html>
